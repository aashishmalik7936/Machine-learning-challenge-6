{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperopt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-HrfdTASAUQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "19632c3f-a360-41a4-e16e-6f5fb31da84f"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lq4ngxHkAam_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive -o nonempty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l-22Y81jAdg3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/hackerearth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o83yBhuuumYd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing the libraries"
      ]
    },
    {
      "metadata": {
        "id": "OQRerfMxAf2u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "00a31a54-0452-43bd-ff44-28980f259860"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from sklearn.cross_validation import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import f1_score\n",
        "import math"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "3w6tmyEdusjs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reading the files"
      ]
    },
    {
      "metadata": {
        "id": "_aE1lo7xAi2u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df=pd.read_csv('train.csv')\n",
        "test_df=pd.read_csv('test.csv')\n",
        "building_use=pd.read_csv('Building_Ownership_Use.csv')\n",
        "building_structure=pd.read_csv('Building_Structure.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NK5ZmUqgu2WK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Merging all the files into a single file"
      ]
    },
    {
      "metadata": {
        "id": "N_s1XGGSA9Nt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "k=pd.merge(building_structure,building_use,on='building_id')\n",
        "train_df=pd.merge(train_df,k,on='building_id')\n",
        "test_df=pd.merge(test_df,k,on='building_id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6GP6DWOHu-bb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "pNJJfBimvDGm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Giving numerical values to **Damage grade**"
      ]
    },
    {
      "metadata": {
        "id": "ijkNFaCPu1EL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Damage Grade\n",
        "p={'Grade 1':0,'Grade 2':1,'Grade 3':2,'Grade 4':3,'Grade 5':4}\n",
        "train_df['damage_grade']=train_df['damage_grade'].map(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GpKoLougvK-e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Filling the **Null Values**"
      ]
    },
    {
      "metadata": {
        "id": "pDkIYJ36BAYY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Has repair started\n",
        "train_df['has_repair_started'].fillna(0.0,inplace=True)\n",
        "test_df['has_repair_started'].fillna(0.0,inplace=True)\n",
        "\n",
        "# Count Families\n",
        "train_df['count_families'].fillna(1.0,inplace=True)\n",
        "test_df['count_families'].fillna(1.0,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZcEcfY9zvQNg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "metadata": {
        "id": "aCNh3F9xvUmy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Making new features by **Pairing two  or more columns**"
      ]
    },
    {
      "metadata": {
        "id": "u0d4qbsqBDra",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Age of building Vdcmun id and mud mortar superstructure wise\n",
        "k=train_df.groupby(['vdcmun_id','has_superstructure_mud_mortar_stone'])['age_building'].mean().reset_index()\n",
        "k=k.rename(columns={'age_building':'age_building_new'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "\n",
        "# Age of buildings where repair has been started\n",
        "k=train_df.groupby(['vdcmun_id','has_repair_started'])['age_building'].mean().reset_index()\n",
        "k=k.rename(columns={'age_building':'age_building_new_2'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "\n",
        "# Age of buildings which are at geotechnical risk\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk'])['age_building'].mean().reset_index()\n",
        "k=k.rename(columns={'age_building':'age_building_new_3'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "\n",
        "# Age of building Vdcmun id and has secondary use wise\n",
        "k=train_df.groupby(['vdcmun_id','has_secondary_use'])['age_building'].mean().reset_index()\n",
        "k=k.rename(columns={'age_building':'age_building_new_4'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "\n",
        "# Age of buildings which are at geotechnical risk of fault cracks\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk_fault_crack'])['age_building'].mean().reset_index()\n",
        "k=k.rename(columns={'age_building':'age_building_new_5'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')\n",
        "\n",
        "# Pre heights of bulidings which are having mud mortar superstructure\n",
        "k=train_df.groupby(['vdcmun_id','has_superstructure_mud_mortar_stone'])['height_ft_pre_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'height_ft_pre_eq':'pre_height_new'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "\n",
        "# Pre heights of buildings which are at geotechnical risk\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk'])['height_ft_pre_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'height_ft_pre_eq':'pre_height_new_2'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "\n",
        "# Pre heights where repairs has been started\n",
        "k=train_df.groupby(['vdcmun_id','has_repair_started'])['height_ft_pre_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'height_ft_pre_eq':'pre_height_new_3'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "\n",
        "# Pre heights of bulidings which are in secondary use\n",
        "k=train_df.groupby(['vdcmun_id','has_secondary_use'])['height_ft_pre_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'height_ft_pre_eq':'pre_height_new_4'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "\n",
        "# Pre heights of buildings which are at geotechnical risk of fault cracks\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk_fault_crack'])['height_ft_pre_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'height_ft_pre_eq':'pre_height_new_5'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')\n",
        "\n",
        "# Post heights of bulidings which are having mud mortar superstructure\n",
        "k=train_df.groupby(['vdcmun_id','has_superstructure_mud_mortar_stone'])['height_ft_post_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'height_ft_post_eq':'post_height_new'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "\n",
        "# Post heights of buildings which are at geotechnical risk\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk'])['height_ft_post_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'height_ft_post_eq':'post_height_new_2'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "\n",
        "# Post heights where repairs has been started\n",
        "k=train_df.groupby(['vdcmun_id','has_repair_started'])['height_ft_post_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'height_ft_post_eq':'post_height_new_3'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "\n",
        "# Post heights of bulidings which are in secondary use\n",
        "k=train_df.groupby(['vdcmun_id','has_secondary_use'])['height_ft_post_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'height_ft_post_eq':'post_height_new_4'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "\n",
        "# Post heights of buildings which are at geotechnical risk of fault cracks\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk_fault_crack'])['height_ft_post_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'height_ft_post_eq':'post_height_new_5'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')\n",
        "\n",
        "# Pre count floors of bulidings which are having mud mortar superstructure\n",
        "k=train_df.groupby(['vdcmun_id','has_superstructure_mud_mortar_stone'])['count_floors_pre_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'count_floors_pre_eq':'pre_count_floor_new'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "\n",
        "# Pre count floors of buildings which are at geotechnical risk\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk'])['count_floors_pre_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'count_floors_pre_eq':'pre_count_floor_new_2'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "\n",
        "# Pre count floors where repairs has been started\n",
        "k=train_df.groupby(['vdcmun_id','has_repair_started'])['count_floors_pre_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'count_floors_pre_eq':'pre_count_floor_new_3'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "\n",
        "# Pre count floors of bulidings which are in secondary use\n",
        "k=train_df.groupby(['vdcmun_id','has_secondary_use'])['count_floors_pre_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'count_floors_pre_eq':'pre_count_floor_new_4'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "\n",
        "# Pre count floors of buildings which are at geotechnical risk of fault cracks\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk_fault_crack'])['count_floors_pre_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'count_floors_pre_eq':'pre_count_floor_new_5'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')\n",
        "\n",
        "# Post count floors of bulidings which are having mud mortar superstructure\n",
        "k=train_df.groupby(['vdcmun_id','has_superstructure_mud_mortar_stone'])['count_floors_post_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'count_floors_post_eq':'post_count_floor_new'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "\n",
        "# Post count floors of buildings which are at geotechnical risk\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk'])['count_floors_post_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'count_floors_post_eq':'post_count_floor_new_2'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "\n",
        "# Post count floors where repairs has been started\n",
        "k=train_df.groupby(['vdcmun_id','has_repair_started'])['count_floors_post_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'count_floors_post_eq':'post_count_floor_new_3'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "\n",
        "# Post count floors of bulidings which are in secondary use\n",
        "k=train_df.groupby(['vdcmun_id','has_secondary_use'])['count_floors_post_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'count_floors_post_eq':'post_count_floor_new_4'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "\n",
        "# Post count floors of buildings which are at geotechnical risk of fault cracks\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk_fault_crack'])['count_floors_post_eq'].mean().reset_index()\n",
        "k=k.rename(columns={'count_floors_post_eq':'post_count_floor_new_5'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')\n",
        "\n",
        "# Plinth area of bulidings which are having mud mortar superstructure\n",
        "k=train_df.groupby(['vdcmun_id','has_superstructure_mud_mortar_stone'])['plinth_area_sq_ft'].mean().reset_index()\n",
        "k=k.rename(columns={'plinth_area_sq_ft':'plinth_area_new'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_superstructure_mud_mortar_stone'],how='left')\n",
        "\n",
        "# Plinth area of buildings which are at geotechnical risk\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk'])['plinth_area_sq_ft'].mean().reset_index()\n",
        "k=k.rename(columns={'plinth_area_sq_ft':'plinth_area_new_2'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk'],how='left')\n",
        "\n",
        "# Plinth area where repairs has been started\n",
        "k=train_df.groupby(['vdcmun_id','has_repair_started'])['plinth_area_sq_ft'].mean().reset_index()\n",
        "k=k.rename(columns={'plinth_area_sq_ft':'plinth_area_new_3'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_repair_started'],how='left')\n",
        "\n",
        "# Plinth area where repairs has been started\n",
        "k=train_df.groupby(['vdcmun_id','has_secondary_use'])['plinth_area_sq_ft'].mean().reset_index()\n",
        "k=k.rename(columns={'plinth_area_sq_ft':'plinth_area_new_4'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_secondary_use'],how='left')\n",
        "\n",
        "# Plinth area of buildings which are at geotechnical risk of fault cracks\n",
        "k=train_df.groupby(['vdcmun_id','has_geotechnical_risk_fault_crack'])['plinth_area_sq_ft'].mean().reset_index()\n",
        "k=k.rename(columns={'plinth_area_sq_ft':'plinth_area_new_5'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','has_geotechnical_risk_fault_crack'],how='left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IgHAxjVcvmBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Generating new columns by **Putting some conditions**"
      ]
    },
    {
      "metadata": {
        "id": "AYB7HewOBJz9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a=train_df['plinth_area_sq_ft']\n",
        "b=test_df['plinth_area_sq_ft']\n",
        "\n",
        "# Condition of buildings post earthquake whose age is more than average age\n",
        "k=train_df.groupby(['vdcmun_id','condition_post_eq'])['age_building'].mean().reset_index()\n",
        "k=k.rename(columns={'age_building':'condition-wise_age'})\n",
        "train_df=train_df.merge(k,on=['vdcmun_id','condition_post_eq'],how='left')\n",
        "test_df=test_df.merge(k,on=['vdcmun_id','condition_post_eq'],how='left')\n",
        "\n",
        "# Condition of buildings post earthquake grouped with count floors\n",
        "## k=train_df.groupby(['vdcmun_id','condition_post_eq'])['count_floors_pre_eq'].mean().reset_index()\n",
        "## k=k.rename(columns={'count_floors_pre_eq':'condition-wise_pre_count_floors'})\n",
        "## train_df=train_df.merge(k,on=['vdcmun_id','condition_post_eq'],how='left')\n",
        "## test_df=test_df.merge(k,on=['vdcmun_id','condition_post_eq'],how='left')\n",
        "\n",
        "# Condition of buildings post earthquake whose post height is more than average height\n",
        "## k=train_df.groupby(['vdcmun_id','condition_post_eq'])['height_ft_post_eq'].mean().reset_index()\n",
        "## k=k.rename(columns={'height_ft_post_eq':'condition-wise_post_height'})\n",
        "## train_df=train_df.merge(k,on=['vdcmun_id','condition_post_eq'],how='left')\n",
        "## test_df=test_df.merge(k,on=['vdcmun_id','condition_post_eq'],how='left')\n",
        "\n",
        "# Creating difference in height as column\n",
        "train_df['difference_in_height']=train_df['height_ft_pre_eq']-train_df['height_ft_post_eq']\n",
        "test_df['difference_in_height']=test_df['height_ft_pre_eq']-test_df['height_ft_post_eq']\n",
        "\n",
        "# Creating difference in floors as column\n",
        "train_df['difference_in_floors']=train_df['count_floors_pre_eq']-train_df['count_floors_post_eq']\n",
        "test_df['difference_in_floors']=test_df['count_floors_pre_eq']-test_df['count_floors_post_eq']\n",
        "\n",
        "# Buildings which require attention\n",
        "train_df.loc[(train_df['condition_post_eq'] == 'Not damaged') & (train_df['age_building'] >= 27.8055) & (train_df['has_repair_started'] == 0),'buildings_require_attention']=1\n",
        "test_df.loc[(test_df['condition_post_eq'] == 'Not damaged') & (test_df['age_building'] >= 27.8055) & (test_df['has_repair_started'] == 0),'buildings_require_attention']=1\n",
        "train_df['buildings_require_attention'].fillna(0,inplace=True)\n",
        "test_df['buildings_require_attention'].fillna(0,inplace=True)\n",
        "\n",
        "# Buildings which require immediate attention\n",
        "train_df.loc[(train_df['condition_post_eq'] == 'Not damaged') & (train_df['height_ft_pre_eq'] >= 16.3089) & (train_df['has_repair_started'] == 0),'buildings_require_immediate_attention']=1\n",
        "test_df.loc[(test_df['condition_post_eq'] == 'Not damaged') & (test_df['height_ft_pre_eq'] >= 16.3089) & (test_df['has_repair_started'] == 0),'buildings_require_immediate_attention']=1\n",
        "train_df['buildings_require_immediate_attention'].fillna(0,inplace=True)\n",
        "test_df['buildings_require_immediate_attention'].fillna(0,inplace=True)\n",
        "\n",
        "# These Buildings also require immediate attention\n",
        "train_df.loc[(train_df['condition_post_eq'] == 'Not damaged') & (train_df['count_floors_pre_eq'] >= 2.1309) & (train_df['has_repair_started'] == 0),'buildings_require_immediate_attention_2']=1\n",
        "test_df.loc[(test_df['condition_post_eq'] == 'Not damaged') & (test_df['count_floors_pre_eq'] >= 2.1309) & (test_df['has_repair_started'] == 0),'buildings_require_immediate_attention_2']=1\n",
        "train_df['buildings_require_immediate_attention_2'].fillna(0,inplace=True)\n",
        "test_df['buildings_require_immediate_attention_2'].fillna(0,inplace=True)\n",
        "\n",
        "# Buildings which were on geotechnical risk but stil not damaged\n",
        "train_df.loc[(train_df['has_geotechnical_risk'] == 1) & (train_df['condition_post_eq'] == 'Not damaged'),'buildings_which_are_lucky']=1\n",
        "test_df.loc[(test_df['has_geotechnical_risk'] == 1) & (test_df['condition_post_eq'] == 'Not damaged'),'buildings_which_are_lucky']=1\n",
        "train_df['buildings_which_are_lucky'].fillna(0,inplace=True)\n",
        "test_df['buildings_which_are_lucky'].fillna(0,inplace=True)\n",
        "\n",
        "# Buildings which are at geotechnical risk but repair hasn't started yet\n",
        "train_df.loc[(train_df['has_geotechnical_risk'] == 1) & (train_df['condition_post_eq'] == 'Not damaged') & (train_df['has_repair_started'] == 0),'govt_is_lazy']=1\n",
        "test_df.loc[(test_df['has_geotechnical_risk'] == 1) & (test_df['condition_post_eq'] == 'Not damaged') & (test_df['has_repair_started'] == 0),'govt_is_lazy']=1\n",
        "train_df['govt_is_lazy'].fillna(0,inplace=True)\n",
        "test_df['govt_is_lazy'].fillna(0,inplace=True)\n",
        "\n",
        "# Buildings with whose post count floors are more than pre count floors\n",
        "cond_train_6=(train_df['difference_in_floors'] >= 0)\n",
        "cond_test_6=(test_df['difference_in_floors'] >= 0)\n",
        "train_df['mistake_in_counting_floors']=np.where(cond_train_6,1,0)\n",
        "test_df['mistake_in_counting_floors']=np.where(cond_test_6,1,0)\n",
        "\n",
        "# Buildings with whose post height is more than pre height\n",
        "cond_train_7=(train_df['difference_in_height'] >= 0)\n",
        "cond_test_7=(test_df['difference_in_height'] >= 0)\n",
        "train_df['mistake_in_measuring_height']=np.where(cond_train_7,1,0)\n",
        "test_df['mistake_in_measuring_height']=np.where(cond_test_7,1,0)\n",
        "\n",
        "# Dividing Plinth area into categories\n",
        "train_df.loc[(train_df['plinth_area_sq_ft']<391.312506),'plinth_area_sq_ft']=5\n",
        "train_df.loc[(train_df['plinth_area_sq_ft']>391.312506) & (train_df['plinth_area_sq_ft']<398.505340),'plinth_area_sq_ft']=4\n",
        "train_df.loc[(train_df['plinth_area_sq_ft']<419.486515) & (train_df['plinth_area_sq_ft']>398.505340),'plinth_area_sq_ft']=3\n",
        "train_df.loc[(train_df['plinth_area_sq_ft']>419.486515) & (train_df['plinth_area_sq_ft']<451.417435),'plinth_area_sq_ft']=2\n",
        "train_df.loc[(train_df['plinth_area_sq_ft']<507.869961) & (train_df['plinth_area_sq_ft']>451.417435),'plinth_area_sq_ft']=1\n",
        "train_df.loc[(train_df['plinth_area_sq_ft']>507.869961),'plinth_area_sq_ft']=0\n",
        "\n",
        "test_df.loc[(test_df['plinth_area_sq_ft']<391.312506),'plinth_area_sq_ft']=5\n",
        "test_df.loc[(test_df['plinth_area_sq_ft']>391.312506) & (test_df['plinth_area_sq_ft']<398.505340),'plinth_area_sq_ft']=4\n",
        "test_df.loc[(test_df['plinth_area_sq_ft']<419.486515) & (test_df['plinth_area_sq_ft']>398.505340),'plinth_area_sq_ft']=3\n",
        "test_df.loc[(test_df['plinth_area_sq_ft']>419.486515) & (test_df['plinth_area_sq_ft']<451.417435),'plinth_area_sq_ft']=2\n",
        "test_df.loc[(test_df['plinth_area_sq_ft']<507.869961) & (test_df['plinth_area_sq_ft']>451.417435),'plinth_area_sq_ft']=1\n",
        "test_df.loc[(test_df['plinth_area_sq_ft']>507.869961),'plinth_area_sq_ft']=0\n",
        "\n",
        "# Retaining Plinth area\n",
        "train_df['plinth_area_sq_ft_new']=a\n",
        "test_df['plinth_area_sq_ft_new']=b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "94ACHee0vzan",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dealing with the categorical columns"
      ]
    },
    {
      "metadata": {
        "id": "UredBz5vBQsF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dummies=['area_assesed','district_id','legal_ownership_status','land_surface_condition','foundation_type','roof_type','ground_floor_type','other_floor_type'\n",
        "        ,'position','plan_configuration','condition_post_eq']\n",
        "for dummy in dummies:\n",
        "  dummies_1 = pd.get_dummies(train_df.loc[:, dummy], prefix=dummy)  \n",
        "  dummies_2 = pd.get_dummies(test_df.loc[:, dummy], prefix=dummy)  \n",
        "  train_df = pd.concat([train_df, dummies_1], axis = 1)\n",
        "  train_df = train_df.drop(dummy, axis =1)\n",
        "  test_df = pd.concat([test_df, dummies_2], axis = 1)\n",
        "  test_df = test_df.drop(dummy, axis =1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9pU6mhDhwBPZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Making test file and train file ready"
      ]
    },
    {
      "metadata": {
        "id": "0eXM6WSVBWc-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_y=train_df['damage_grade'].values\n",
        "test_id=test_df['building_id'].values\n",
        "train_X=train_df.drop(['building_id','damage_grade','ward_id_x','ward_id_y','vdcmun_id','vdcmun_id_x','vdcmun_id_y','district_id_x','district_id_y'],axis=1)\n",
        "test_X=test_df.drop(['building_id','ward_id_x','ward_id_y','vdcmun_id','vdcmun_id_x','vdcmun_id_y','district_id_x','district_id_y'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kftnax0xwGhk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dropping unwanted features with the help of feature importance given by lightgbm"
      ]
    },
    {
      "metadata": {
        "id": "oc3tpoRIv-ni",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "col_to_drop=['condition_post_eq_Damaged-Rubble Clear-New building built','condition_post_eq_Damaged-Rubble unclear','buildings_require_attention'\n",
        "             ,'condition_post_eq_Damaged-Rubble clear','has_secondary_use_gov_office']\n",
        "train_X=train_X.drop(col_to_drop,axis=1)\n",
        "test_X=test_X.drop(col_to_drop,axis=1)\n",
        "\n",
        "# Filling null values of test file\n",
        "test_X['age_building_new'].fillna(27.805518,inplace=True)\n",
        "test_X['age_building_new_2'].fillna(27.805518,inplace=True)\n",
        "test_X['age_building_new_3'].fillna(27.805518,inplace=True)\n",
        "test_X['age_building_new_4'].fillna(27.805518,inplace=True)\n",
        "test_X['age_building_new_5'].fillna(27.805518,inplace=True)\n",
        "test_X['condition-wise_age'].fillna(27.805518,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LHUFSLmewVvW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install the required libraries"
      ]
    },
    {
      "metadata": {
        "id": "oMqIG0aHBZ3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7b594e96-32f9-4de0-85e6-372d12eab7e4"
      },
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n",
        "import lightgbm as lgb"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.19.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.14.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.19.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CcctJFrmDk-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0e88f87c-de25-46e1-a9fe-69583e3f2b0b"
      },
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/83/3d/d2dc2abaef016c597febfce67c6868ee4c3d1b81e9b9e4f0b3ad551a3625/bayesian-optimization-0.6.0.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.14.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.19.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.19.2)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Running setup.py bdist_wheel for bayesian-optimization ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/f5/ef/5c/9ff1d2d86ad1117bf4cd3f989356ae46fa907014ece63b09dd\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mckCMkgYDnu-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dffdc56e-1ef3-4dcd-a8c8-c37cb9af1f9e"
      },
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold, KFold"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.5.2)\r\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.19.2)\r\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.19.1)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.14.5)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "70BTavFgwbot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training lightgbm with scikit optimzation"
      ]
    },
    {
      "metadata": {
        "id": "W29m2ynVEJHg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def status_print(optim_result):\n",
        "    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n",
        "    \n",
        "    # Get all the models tested so far in DataFrame format\n",
        "    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n",
        "    \n",
        "    # Get current parameters and the best parameters    \n",
        "    best_params = pd.Series(bayes_cv_tuner.best_params_)\n",
        "    print('Model #{}\\nBest MSE: {}\\nBest params: {}\\n'.format(\n",
        "        len(all_models),\n",
        "        np.round(bayes_cv_tuner.best_score_, 4),\n",
        "        bayes_cv_tuner.best_params_\n",
        "    ))\n",
        "    \n",
        "    # Save all model results\n",
        "    clf_name = bayes_cv_tuner.estimator.__class__.__name__\n",
        "    all_models.to_csv(clf_name+\"_cv_results.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9zyDaQAvJxpT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bayes_cv_tuner = BayesSearchCV(\n",
        "    estimator = lgb.LGBMClassifier(objective='multiclass', boosting_type='gbdt',\n",
        "        learning_rate = 0.022066991249460103,\n",
        "        sub_sample = 0.7048800081190388,\n",
        "        num_leaves = 443,      \n",
        "        max_depth = 20,\n",
        "        colsample_bytree = 0.42149456283334996 ,\n",
        "        subsample = 0.16628951140171316,\n",
        "        min_child_samples = 26,\n",
        "        max_bin= 1018,\n",
        "        subsample_freq= 0,\n",
        "        min_child_weight= 1,\n",
        "        reg_lambda= 4.3806965488564525e-05,\n",
        "        reg_alpha= 0.1611980387486336,\n",
        "        scale_pos_weight= 0.0009365503147654213,\n",
        "        n_estimators= 792,\n",
        "                                  ),   \n",
        "    search_spaces={'subsample_freq':(0,1)},\n",
        "    scoring = 'f1_weighted', #neg_mean_squared_log_error\n",
        "    cv = KFold(\n",
        "        n_splits=6,\n",
        "        shuffle=True,\n",
        "        random_state=42\n",
        "    ),\n",
        "    n_jobs = 1,\n",
        "    n_iter = 1,   \n",
        "    verbose = 20,\n",
        "    refit = True,\n",
        "    random_state = 42\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TqgaCg8rJ43F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "261d7b4f-8d2a-4ced-9650-917943f7c4ab"
      },
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "result = bayes_cv_tuner.fit(train_X, train_y, callback=status_print)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n",
            "[CV] subsample_freq=0 ................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 23.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... subsample_freq=0, score=0.7835320592203386, total=23.0min\n",
            "[CV] subsample_freq=0 ................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 45.4min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... subsample_freq=0, score=0.7830141366727436, total=22.4min\n",
            "[CV] subsample_freq=0 ................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 68.4min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... subsample_freq=0, score=0.7842193528234515, total=23.0min\n",
            "[CV] subsample_freq=0 ................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n",
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 91.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ........ subsample_freq=0, score=0.784872862659364, total=22.7min\n",
            "[CV] subsample_freq=0 ................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 113.7min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... subsample_freq=0, score=0.7840262343210925, total=22.6min\n",
            "[CV] subsample_freq=0 ................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n",
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 136.0min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... subsample_freq=0, score=0.7836501723224234, total=22.3min\n",
            "[CV] subsample_freq=0 ................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n",
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 158.9min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... subsample_freq=0, score=0.7821757017430627, total=22.9min\n",
            "[CV] subsample_freq=0 ................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n",
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 181.4min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 181.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ....... subsample_freq=0, score=0.7816420500710872, total=22.6min\n",
            "Model #1\n",
            "Best MSE: 0.7834\n",
            "Best params: {'subsample_freq': 0}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SHS5MQ4z6oIS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = bayes_cv_tuner.predict(test_X)\n",
        "#pred_lgb = np.asarray([np.argmax(line) for line in pred])\n",
        "pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qUhMtjzzKDjd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submit_df=pd.DataFrame({'building_id':test_id})\n",
        "submit_df['damage_grade']=pred\n",
        "p={0:'Grade 1',1:'Grade 2',2:'Grade 3',3:'Grade 4',4:'Grade 5'}\n",
        "submit_df['damage_grade']=submit_df['damage_grade'].map(p)\n",
        "submit_df['damage_grade'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "niw2crU25hHS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exporting the predictions to csv files"
      ]
    },
    {
      "metadata": {
        "id": "g2OxW6tBwpEC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submit_df.to_csv('hackerearth_submission_141.csv',index=False) #75.453\n",
        "from google.colab import files\n",
        "downloaded=files.download('hackerearth_submission_141.csv') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Efs2Gx1y4bkB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}